{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "from notebookutils import mssparkutils\n",
        "\n",
        "data_lake_account_name = '' # Synapse Workspace ADLS\n",
        "file_system_name = '' # Synapse Workspace ADLS container\n",
        "\n",
        "keyvault_name = '' # Key Vault Name\n",
        "\n",
        "query = '' #change this to your query string\n",
        "topic = '' #change this to your topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "base_path = f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/\"\n",
        "\n",
        "df = spark.read.load(base_path + 'CountryCoordinates/CountryCordinates.csv', format='csv', header=True)\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"country_coordinates\")\n",
        "# display(df.take(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "mssparkutils.notebook.run(\"/Ingest_Process_News\", 1800, \\\n",
        "{\"data_lake_account_name\": data_lake_account_name, \"file_system_name\": file_system_name, \"keyvault_name\": keyvault_name, \"query\": query, \"topic\": topic})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "mssparkutils.notebook.run(\"/Ingest_Process_Tweets\", 1800, \\\n",
        " {\"data_lake_account_name\": data_lake_account_name, \"file_system_name\": file_system_name,\"keyvault_name\": keyvault_name, \"query\": query, \"topic\": topic})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Code snippets to clean up duplicates if Power BI refresh shows any errors\n",
        "\n",
        "df_articles = spark.sql('select * from articles_data')\n",
        "df_articles = df_articles.dropDuplicates(['id'])\n",
        "df_articles.write.mode(\"overwrite\").saveAsTable(\"articles_data_temp\")\n",
        "\n",
        "df_articles = spark.sql('select * from articles_data_temp')\n",
        "df_articles.write.mode(\"overwrite\").saveAsTable(\"articles_data\")\n",
        "\n",
        "df = spark.sql('select * from tweets_users_data')\n",
        "df = df.dropDuplicates(['id'])\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"tweets_users_data_temp\")\n",
        "\n",
        "df = spark.sql('select * from tweets_users_data_temp')\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"tweets_users_data\")\n",
        "\n",
        "df = spark.sql('select * from tweets_sentiments')\n",
        "df = df.dropDuplicates(['id'])\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"tweets_sentiments_temp\")\n",
        "\n",
        "df = spark.sql('select * from tweets_sentiments_temp')\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"tweets_sentiments\")\n",
        "\n",
        "df_tweets = spark.sql('select * from tweets_data')\n",
        "df_tweets = df_tweets.dropDuplicates(['originalid'])\n",
        "df_tweets.write.mode(\"overwrite\").saveAsTable(\"tweets_data_temp\")\n",
        "\n",
        "df_tweets = spark.sql('select * from tweets_data_temp')\n",
        "df_tweets.write.mode(\"overwrite\").saveAsTable(\"tweets_data\")\n",
        "\n",
        "f_tweets = spark.sql('select * from tweets_data')\n",
        "df_tweets = df_tweets.dropDuplicates(['id'])\n",
        "df_tweets.write.mode(\"overwrite\").saveAsTable(\"tweets_data_temp\")\n",
        "\n",
        "df_tweets = spark.sql('select * from tweets_data_temp')\n",
        "df_tweets.write.mode(\"overwrite\").saveAsTable(\"tweets_data\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
