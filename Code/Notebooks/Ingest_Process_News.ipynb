{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "data_lake_account_name = ''\n",
        "file_system_name = ''\n",
        "\n",
        "keyvault_name = ''\n",
        "\n",
        "query = ''\n",
        "topic = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# News API\n",
        "NEWS_API_KEY = TokenLibrary.getSecret(keyvault_name,\"NEWSAPIKEY\",\"KeyVaultLinkedService\")\n",
        "# Text analytics\n",
        "TEXT_ANALYTICS_KEY = TokenLibrary.getSecret(keyvault_name,\"TEXTANALYTICSKEY\",\"KeyVaultLinkedService\")\n",
        "TEXT_ANALYTICS_ENDPOINT = TokenLibrary.getSecret(keyvault_name,\"TEXTANALYTICSENDPOINT\",\"KeyVaultLinkedService\")\n",
        "TEXT_ANALYTICS_REGION = TokenLibrary.getSecret(keyvault_name,\"TEXTANALYTICSREGION\",\"KeyVaultLinkedService\")\n",
        "# Translator\n",
        "TRANSLATOR_KEY = TokenLibrary.getSecret(keyvault_name,\"TRANSLATORKEY\",\"KeyVaultLinkedService\")\n",
        "TRANSLATOR_ENDPOINT = TokenLibrary.getSecret(keyvault_name,\"TRANSLATORENDPOINT\",\"KeyVaultLinkedService\")\n",
        "TRANSLATOR_REGION = TokenLibrary.getSecret(keyvault_name,\"TRANSLATORREGION\",\"KeyVaultLinkedService\")\n",
        "# Twitter\n",
        "TWITTER_API_KEY = TokenLibrary.getSecret(keyvault_name,\"TWITTERAPIKEY\",\"KeyVaultLinkedService\")\n",
        "TWITTER_API_SECRET_KEY = TokenLibrary.getSecret(keyvault_name,\"TWITTERAPISECRETKEY\",\"KeyVaultLinkedService\")\n",
        "TWITTER_ACCESS_TOKEN = TokenLibrary.getSecret(keyvault_name,\"TWITTERACCESSTOKEN\",\"KeyVaultLinkedService\")\n",
        "TWITTER_ACCESS_TOKEN_SECRET = TokenLibrary.getSecret(keyvault_name,\"TWITTERACCESSTOKENSECRET\",\"KeyVaultLinkedService\")\n",
        "TWITTER_API_AUTH = {\n",
        "  'consumer_key': TWITTER_API_KEY,\n",
        "  'consumer_secret': TWITTER_API_SECRET_KEY,\n",
        "  'access_token': TWITTER_ACCESS_TOKEN,\n",
        "  'access_token_secret': TWITTER_ACCESS_TOKEN_SECRET,\n",
        "}\n",
        "# Azure Maps\n",
        "MAPS_KEY = TokenLibrary.getSecret(keyvault_name,\"MAPSKEY\",\"KeyVaultLinkedService\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, date, timedelta # don't import time here. It messes with the default library\n",
        "base_path = f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/\"\n",
        "foldername_suffix = (datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "if topic == '':\n",
        "    topic = query\n",
        "subtopic = \"\"\n",
        "\n",
        "language = \"All\"\n",
        "target_languages = \"English,Spanish\"\n",
        "\n",
        "from_date = \"\"\n",
        "to_date = \"\"\n",
        "\n",
        "sort_by = \"popularity\" # popularity, relevancy, publishedAt\n",
        "qInTitle = \"\" \n",
        "page_size = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import urllib.parse\n",
        "import sys, time, json, requests, uuid\n",
        "from azure.cosmos import CosmosClient\n",
        "\n",
        "from dateutil.parser import parse\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "COUNTRIES = [\"ae\", \"ar\", \"at\", \"au\", \"be\", \"bg\", \"br\", \"ca\", \"ch\", \"cn\", \"co\", \"cu\", \"cz\", \"de\", \"eg\", \"fr\", \"gb\", \"gr\", \"hk\", \"hu\", \"id\", \"ie\", \"il\", \"in\", \"it\", \"jp\", \"kr\", \"lt\", \"lv\", \"ma\", \"mx\", \"my\", \"ng\", \"nl\", \"no\", \"nz\", \"ph\", \"pl\", \"pt\", \"ro\", \"rs\", \"ru\", \"sa\", \"se\", \"sg\", \"si\", \"sk\", \"th\", \"tr\", \"tw\", \"ua\", \"us\", \"ve\", \"za\"]\n",
        "LANGUAGE_CODES = {\n",
        "  \"All\": \"\",\n",
        "  \"Afrikaans\":\"af\",\n",
        "  \"Arabic\":\"ar\",\n",
        "  \"Assamese\":\"as\",\n",
        "  \"Bangla\":\"bn\",\n",
        "  \"Bosnian(Latin)\":\"bs\",\n",
        "  \"Bulgarian\":\"bg\",\n",
        "  \"Cantonese(Traditional)\":\"yue\",\n",
        "  \"Catalan\":\"ca\",\n",
        "  \"Chinese Simplified\":\"zh-Hans\",\n",
        "  \"Chinese Traditional\":\"zh-Hant\",\n",
        "  \"Croatian\":\"hr\",\n",
        "  \"Czech\":\"cs\",\n",
        "  \"Dari\":\"prs\",\n",
        "  \"Danish\":\"da\",\n",
        "  \"Dutch\":\"nl\",\n",
        "  \"English\":\"en\",\n",
        "  \"Estonian\":\"et\",\n",
        "  \"Fijian\":\"fj\",\n",
        "  \"Filipino\":\"fil\",\n",
        "  \"Finnish\":\"fi\",\n",
        "  \"French\":\"fr\",\n",
        "  \"German\":\"de\",\n",
        "  \"Greek\":\"el\",\n",
        "  \"Gujarati\":\"gu\",\n",
        "  \"Haitian Creole\":\"ht\",\n",
        "  \"Hebrew\":\"he\",\n",
        "  \"Hindi\":\"hi\",\n",
        "  \"Hmong Daw\":\"mww\",\n",
        "  \"Hungarian\":\"hu\",\n",
        "  \"Icelandic\":\"is\",\n",
        "  \"Indonesian\":\"id\",\n",
        "  \"Irish\":\"ga\",\n",
        "  \"Italian\":\"it\",\n",
        "  \"Japanese\":\"ja\",\n",
        "  \"Kannada\":\"kn\",\n",
        "  \"Kazakh\":\"kk\",\n",
        "  \"Klingon\":\"tlh-Latn\",\n",
        "  \"Klingon(plqaD)\":\"tlh-Piqd\",\n",
        "  \"Korean\":\"ko\",\n",
        "  \"Kurdish(Central)\":\"ku\",\n",
        "  \"Kurdish(Northern)\":\"kmr\",\n",
        "  \"Latvian\":\"lv\",\n",
        "  \"Lithuanian\":\"lt\",\n",
        "  \"Malagasy\":\"mg\",\n",
        "  \"Malay\":\"ms\",\n",
        "  \"Malayalam\":\"ml\",\n",
        "  \"Maltese\":\"mt\",\n",
        "  \"Maori\":\"mi\",\n",
        "  \"Marathi\":\"mr\",\n",
        "  \"Norwegian\":\"nb\",\n",
        "  \"Odia\":\"or\",\n",
        "  \"Pashto\":\"ps\",\n",
        "  \"Persian\":\"fa\",\n",
        "  \"Polish\":\"pl\",\n",
        "  \"Portuguese(Brazil)\":\"pt-br\",\n",
        "  \"Portuguese(Portugal)\":\"pt-pt\",\n",
        "  \"Punjabi\":\"pa\",\n",
        "  \"Queretaro Otomi\":\"otq\",\n",
        "  \"Romanian\":\"ro\",\n",
        "  \"Russian\":\"ru\",\n",
        "  \"Samoan\":\"sm\",\n",
        "  \"Serbian(Cyrillic)\":\"sr-Cyrl\",\n",
        "  \"Serbian(Latin)\":\"sr-Latn\",\n",
        "  \"Slovak\":\"sk\",\n",
        "  \"Slovenian\":\"sl\",\n",
        "  \"Spanish\":\"es\",\n",
        "  \"Swahili\":\"sw\",\n",
        "  \"Swedish\":\"sv\",\n",
        "  \"Tahitian\":\"ty\",\n",
        "  \"Tamil\":\"ta\",\n",
        "  \"Telugu\":\"te\",\n",
        "  \"Thai\":\"th\",\n",
        "  \"Tongan\":\"to\",\n",
        "  \"Turkish\":\"tr\",\n",
        "  \"Ukrainian\":\"uk\",\n",
        "  \"Urdu\":\"ur\",\n",
        "  \"Vietnamese\":\"vi\",\n",
        "  \"Welsh\":\"cy\",\n",
        "  \"Yucatec Maya\":\"yua\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "from_date = ''\n",
        "\n",
        "try:\n",
        "    sql_str = 'select max(published_date) from articles_data where query =  ' + '\\'' + str(query) + '\\''\n",
        "    df_maxdate = spark.sql(sql_str).first()\n",
        "    from_date = df_maxdate[0]\n",
        "except:\n",
        "    from_date = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "config = {}\n",
        "config[\"q\"] = query\n",
        "language_codes = [LANGUAGE_CODES.get(key) for key in language.split(\",\") if key != \"All\"]\n",
        "if language_codes:\n",
        "    if language_codes[0] in [\"ar\",\"de\",\"en\",\"es\",\"fr\",\"he\",\"it\",\"nl\",\"no\",\"pt\",\"ru\",\"se\",\"ud\",\"zh\"]:\n",
        "        config[\"language\"] = language_codes[0]# ','.join(language_codes) # take the first language only as the API does not support multiple values\n",
        "config[\"from\"] = from_date\n",
        "config[\"to\"] = to_date\n",
        "config[\"sortBy\"] = sort_by\n",
        "config[\"qInTitle\"] = qInTitle\n",
        "config[\"pageSize\"] = page_size\n",
        "config[\"apiKey\"] = NEWS_API_KEY\n",
        "target_languages = [LANGUAGE_CODES.get(lang, \"\") for lang in target_languages.split(\",\")]\n",
        "if \"en\" not in target_languages:\n",
        "    target_languages.append(\"en\") # always include english in target languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "def build_query(config, page=1):\n",
        "    query = \"http://newsapi.org/v2/everything?\"\n",
        "    for k,v in config.items():\n",
        "        if v is None or v == \"\":\n",
        "            continue\n",
        "        if k is 'q':\n",
        "            v = v.replace(\"#\",\"\")\n",
        "            v = urllib.parse.quote_plus(v)\n",
        "            query += f\"{k}={v}&\"\n",
        "            continue        \n",
        "        query += f\"{k}={v}&\"\n",
        "    query += f\"page={page}\"\n",
        "    return query\n",
        "\n",
        "\n",
        "query = build_query(config)\n",
        "print(query)\n",
        "\n",
        "page = 1\n",
        "all_articles = []\n",
        "while True:\n",
        "    url = build_query(config, page)\n",
        "    print(f\"Getting page {page}...\")\n",
        "    response = requests.get(url).json()\n",
        "    articles = response.get(\"articles\")\n",
        "    if response[\"status\"] == \"error\":\n",
        "        break\n",
        "    if not articles:\n",
        "        break\n",
        "    if page > page_size: \n",
        "        break\n",
        "\n",
        "    all_articles.extend(articles)\n",
        "    \n",
        "    page += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_articles = spark.createDataFrame(Row(**x) for x in all_articles)\n",
        "file_path = base_path + 'NewsRawData/' + config[\"q\"]  + '_' + str(topic) + '_' + foldername_suffix\n",
        "df_articles.write.format('json').save(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "file_path = base_path + 'NewsRawData/' + config[\"q\"]  + '_' + str(topic) + '_' + foldername_suffix\n",
        "articles_df = spark.read.load(file_path, format='json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse\n",
        "import hashlib \n",
        "\n",
        "def get_month_year(publishedAt): \n",
        "    return (str(parse(publishedAt).month) + \"_\"+str(parse(publishedAt).year))\n",
        "month_year_udf = udf(lambda publishedAt: get_month_year(publishedAt), returnType=StringType())\n",
        "\n",
        "def get_domain(url): \n",
        "    return (urlparse(url).netloc)\n",
        "domain_udf = udf(lambda url: get_domain(url), returnType=StringType())\n",
        "\n",
        "def get_id(url,query): \n",
        "    return(hashlib.sha256((str(url) + str(query)).encode('utf-8')).hexdigest())\n",
        "id_udf = udf(lambda url,query: get_id(url,query), returnType=StringType())\n",
        "\n",
        "def get_title(title,content): \n",
        "    titleLength = len(content)\n",
        "    if titleLength>80:\n",
        "        titleLength=79\n",
        "    if title is None:\n",
        "        title=content[0:titleLength]\n",
        "    \n",
        "    title=title[0].upper()+title[1:]\n",
        "    i=title.find(' ', titleLength)\n",
        "    if i==-1:\n",
        "        title=title\n",
        "    else:\n",
        "        title=title[0:i].replace(' ,',',').rstrip(' ').rstrip(',')\n",
        "    return(title)\n",
        "title_udf = udf(lambda title,content: get_title(title,content), returnType=StringType())\n",
        "\n",
        "def get_inserted_date(publishedAt):\n",
        "    dt2 = datetime.now()\n",
        "    ts= int(time.mktime(dt2.timetuple()))\n",
        "    at=dt2.strftime(\"%m/%d/%Y, %H:%M:%S %Z\")\n",
        "    return(at)\n",
        "inserted_date_udf = udf(lambda publishedAt: get_inserted_date(publishedAt), returnType=StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df = articles_df.select(['url','author','urlToImage','description','content', 'publishedAt','title' \\\n",
        "                        ,col(\"source.Name\").alias(\"sourceName\")]) \\\n",
        "                .withColumn(\"query\", lit(config[\"q\"])) \\\n",
        "                .withColumn(\"topickey\", lit(topic)) \\\n",
        "                .withColumn(\"subtopic\", lit(subtopic)) \\\n",
        "                .withColumn(\"document_type\", lit('news_article')) \\\n",
        "                .withColumn('month_year', month_year_udf(lower(col(\"publishedAt\")))) \\\n",
        "                .withColumn('title', title_udf(col(\"title\"),col(\"content\"))) \\\n",
        "                .withColumn('domainname', domain_udf(col(\"url\"))) \\\n",
        "                .withColumn('id', id_udf(col(\"url\"),lit(config[\"q\"]))) \\\n",
        "                .withColumn('inserted_datetime', inserted_date_udf(col(\"publishedAt\"))) \\\n",
        "                .withColumn('published_date',to_date(to_timestamp(col(\"publishedAt\"),\"yyyy-MM-dd'T'HH:mm:ss'Z'\"),'yyyy-MM-dd'))\n",
        "               \n",
        "                \n",
        "df.write.mode(\"append\").saveAsTable(\"articles_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sys\n",
        "import copy\n",
        "import tweepy\n",
        "import urllib.parse\n",
        "import sys, time, json, requests, uuid\n",
        "from tweepy import API\n",
        "from tweepy import Cursor\n",
        "from tweepy import OAuthHandler\n",
        "from dateutil.parser import parse\n",
        "from datetime import datetime, date, timedelta # don't import time here. It messes with the default library\n",
        "from azure.cosmos import CosmosClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "\n",
        "def get_translation(inp_text, to_languages):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "    inp_text: text to be translated\n",
        "    to_languages: list of languages to translate to\n",
        "    Returns: {lang_code: translation}, language code of the original text\n",
        "    Call to translator cognitive service detects language and translates to the target languages. \n",
        "    Result is a dictionary of language codes to translated text, along with the language detected.\n",
        "    \"\"\"\n",
        "    # Translator setup\n",
        "    translator_path = \"/translate\"\n",
        "    translator_url = TRANSLATOR_ENDPOINT + translator_path\n",
        "    params = {\n",
        "    \"api-version\": \"3.0\",\n",
        "    \"to\": to_languages\n",
        "    }\n",
        "    headers = {\n",
        "    'Ocp-Apim-Subscription-Key': TRANSLATOR_KEY,\n",
        "    'Ocp-Apim-Subscription-Region': TRANSLATOR_REGION,\n",
        "    'Content-type': 'application/json',\n",
        "    'X-ClientTraceId': str(uuid.uuid4())\n",
        "    }\n",
        "    # create and send request\n",
        "    body = [{\n",
        "    'text': inp_text\n",
        "    }]\n",
        "    request = requests.post(translator_url, params=params, headers=headers, json=body)\n",
        "    response = request.json()\n",
        "    \n",
        "    try:\n",
        "        from_language = response[0][\"detectedLanguage\"][\"language\"]\n",
        "        translations = response[0][\"translations\"]\n",
        "        res = {} \n",
        "        for trans in translations:\n",
        "            res[trans['to']] = trans['text']\n",
        "            return res, from_language \n",
        "    except Exception as err:\n",
        "        print(\"Encountered an exception. {}\".format(err))\n",
        "        return err\n",
        "\n",
        "def authenticate_client():\n",
        "    \"\"\"\n",
        "    Returns: text analytics client\n",
        "    \"\"\"\n",
        "    ta_credential = AzureKeyCredential(TEXT_ANALYTICS_KEY)\n",
        "    text_analytics_client = TextAnalyticsClient(\n",
        "            endpoint=TEXT_ANALYTICS_ENDPOINT, \n",
        "            credential=ta_credential)\n",
        "    return text_analytics_client\n",
        "text_analytics_client = authenticate_client()\n",
        "\n",
        "def get_sentiment(inp_text):\n",
        "    documents = [inp_text]\n",
        "    response = text_analytics_client.analyze_sentiment(documents = documents)[0]  \n",
        "    try:\n",
        "        overallscore = response.confidence_scores.positive + (0.5*response.confidence_scores.neutral) \n",
        "        return response.sentiment, overallscore\n",
        "    except Exception as err:\n",
        "        print(\"Encountered Sentiment exception. {}\".format(err))\n",
        "        return \"Neutral\",0.5\n",
        "def get_ner(inp_text):\n",
        "    try:\n",
        "        documents = [inp_text]\n",
        "        result = text_analytics_client.recognize_entities(documents = documents)[0]  \n",
        "        return [{\"text\": x.text, \"category\": x.category, \"subcategory\": x.subcategory, \"length\": x.length, \"offset\": x.offset, \"confidence_score\": x.confidence_score} for x in result.entities]\n",
        "    except Exception as err:\n",
        "        print(\"Encountered NER exception. {}\".format(err))\n",
        "    return []\n",
        "def get_key_phrases(inp_text):\n",
        "    try:\n",
        "      documents = [inp_text]\n",
        "      response = text_analytics_client.extract_key_phrases(documents = documents)[0] \n",
        "      if not response.is_error:\n",
        "          return response.key_phrases\n",
        "      else:\n",
        "          print(response.id, response.error)\n",
        "    except Exception as err:\n",
        "      print(\"Encountered Translation exception. {}\".format(err))\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "from pyspark.sql import Row\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Translations\", MapType(StringType(),StringType()), False),\n",
        "    StructField(\"QueryLanguage\", StringType(), False)])\n",
        "\n",
        "def get_translations(originaltext,target_languages): \n",
        "    translations_text, query_language = get_translation(originaltext, ast.literal_eval(target_languages))\n",
        "    return Row('Translations', 'QueryLanguage')(translations_text, query_language)\n",
        "\n",
        "get_translations_udf = udf(lambda originaltext,target_languages: get_translations(originaltext,target_languages),returnType=schema)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_translations = df.select(['id','title','description','content']) \\\n",
        "                .withColumn('Ttitle1', get_translations_udf(col(\"title\"),lit(str(target_languages)))) \\\n",
        "                .withColumn('original_language', col(\"Ttitle1.QueryLanguage\")) \\\n",
        "                .select(explode(col(\"Ttitle1.Translations\")).alias(\"language\",\"TText\"),'id','title','description','content','original_language') \\\n",
        "                .withColumn(\"field\", lit('Title')) \\\n",
        "                .withColumn('created_datetime', inserted_date_udf(col(\"id\"))) \n",
        "\n",
        "df_translations = df_translations.union(df_translations.select('*') \\\n",
        "                .withColumn('TDescr1', get_translations_udf(col(\"description\"),lit(str(target_languages)))) \\\n",
        "                .withColumn('original_language', col(\"TDescr1.QueryLanguage\")) \\\n",
        "                .select(explode(col(\"TDescr1.Translations\")).alias(\"language\",\"TText\"),'id','title','description','content','original_language') \\\n",
        "                .withColumn(\"field\", lit('Description')) \\\n",
        "                .withColumn('created_datetime', inserted_date_udf(col(\"id\"))) \n",
        "                )\n",
        "df_translations = df_translations.union(df_translations.select('*') \\\n",
        "                .withColumn('TContent1', get_translations_udf(col(\"content\"),lit(str(target_languages)))) \\\n",
        "                .withColumn('original_language', col(\"TContent1.QueryLanguage\")) \\\n",
        "                .select(explode(col(\"TContent1.Translations\")).alias(\"language\",\"TText\"),'id','title','description','content','original_language') \\\n",
        "                .withColumn(\"field\", lit('Content')) \\\n",
        "                .withColumn('created_datetime', inserted_date_udf(col(\"id\"))) \n",
        "                )\n",
        "df_translations = df_translations.select('id','title','description','content','original_language','created_datetime','language','field','TText')\n",
        "\n",
        "df_translations.write.mode(\"append\").saveAsTable(\"articles_translations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "schema = MapType(StringType(),ArrayType(MapType(StringType(),StringType())))\n",
        "        \n",
        "def get_ners(original_language,language,content,TContent):\n",
        "    named_entities = []\n",
        "    if language != \"en\":\n",
        "        if TContent !='':\n",
        "            named_entities = get_ner(content)\n",
        "            named_entity_obj = {language: named_entities}\n",
        "        else:\n",
        "            named_entity_obj = {language: named_entities}\n",
        "    else:\n",
        "        named_entities = get_ner(content) \n",
        "        named_entity_obj = {\"en\": named_entities}\n",
        "    return(named_entity_obj)\n",
        "\n",
        "get_ner_udf = udf(lambda original_language,language,content,TContent: get_ners(original_language,language,content,TContent),returnType=schema)\n",
        "\n",
        "df_ner = df_translations.filter(df_translations.field == 'Content') \\\n",
        "                .select(['id','original_language','language','content','TText']) \\\n",
        "                .withColumn('NER', get_ner_udf(col(\"original_language\"),col(\"language\"),col(\"content\"),col(\"TText\"))) \\\n",
        "                .withColumn('created_datetime', inserted_date_udf(col(\"id\"))) \\\n",
        "                .select(explode(col(\"NER\")).alias(\"language\",\"ner1\"),'id','original_language','created_datetime') \\\n",
        "                .select(explode(col(\"ner1\")).alias(\"ner2\"),'id','created_datetime','language') \\\n",
        "                .select(['id','created_datetime','language',col(\"ner2.text\").alias(\"value\"),col(\"ner2.subcategory\").alias(\"subcategory\"),col(\"ner2.offset\").alias(\"offset\"),\n",
        "                col(\"ner2.confidence_score\").alias(\"confidence_score\"), col(\"ner2.category\").alias(\"category\"),col(\"ner2.length\").alias(\"length\")])\n",
        "\n",
        "df_ner.write.mode(\"append\").saveAsTable(\"articles_entities\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "from pyspark.sql import Row\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"sentiment\", StringType(), False),\n",
        "    StructField(\"sentiment_score\", DecimalType(), False)])\n",
        "\n",
        "def get_sentiments(original_language,title,TText): \n",
        "    if original_language != \"en\":\n",
        "        sentiment, sentiment_score = get_sentiment(TText)\n",
        "    else:\n",
        "        sentiment, sentiment_score = get_sentiment(title)\n",
        "    return Row('sentiment', 'sentiment_score')(sentiment, sentiment_score)\n",
        "\n",
        "get_sentiments_udf = udf(lambda originaltext,title,TText: get_sentiments(originaltext,title,TText),returnType=schema)\n",
        "\n",
        "df_sentiments = df_translations.filter(df_translations.field == 'Title') \\\n",
        "                .select(['id','original_language','title','TText']) \\\n",
        "                .withColumn('TS1', get_sentiments_udf(col(\"original_language\"),col(\"title\"),col(\"TText\"))) \\\n",
        "                .withColumn('sentiment', col(\"TS1.sentiment\")) \\\n",
        "                .withColumn('sentiment_score', col(\"TS1.sentiment_score\")) \\\n",
        "                .withColumn('created_datetime', inserted_date_udf(col(\"id\"))) \\\n",
        "                .select(['id','sentiment','sentiment_score','created_datetime'])\n",
        "                \n",
        "df_sentiments.write.mode(\"append\").saveAsTable(\"articles_sentiments\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
