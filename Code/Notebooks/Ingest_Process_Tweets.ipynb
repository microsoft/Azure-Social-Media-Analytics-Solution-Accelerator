{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation.\r\n",
        "\r\n",
        "Licensed under the MIT License."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_lake_account_name = ''\n",
        "file_system_name = ''\n",
        "\n",
        "keyvault_name = ''\n",
        "\n",
        "query = ''\n",
        "topic = ''"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if topic == '':\n",
        "    topic = query\n",
        "subtopic = \"\"\n",
        "query_language = \"All\"\n",
        "target_languages = \"English,Spanish\"\n",
        "num_tweets = 100\n",
        "days = 7\n",
        "\n",
        "language=\"en\" \n",
        "maxdays=365\n",
        "maxtweets_persearch=100"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, date, timedelta # don't import time here. It messes with the default library\n",
        "base_path = f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/\"\n",
        "foldername_suffix = (datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# News API\n",
        "NEWS_API_KEY = TokenLibrary.getSecret(keyvault_name,\"NEWSAPIKEY\",\"KeyVaultLinkedService\")\n",
        "# Text analytics\n",
        "TEXT_ANALYTICS_KEY = TokenLibrary.getSecret(keyvault_name,\"TEXTANALYTICSKEY\",\"KeyVaultLinkedService\")\n",
        "TEXT_ANALYTICS_ENDPOINT = TokenLibrary.getSecret(keyvault_name,\"TEXTANALYTICSENDPOINT\",\"KeyVaultLinkedService\")\n",
        "TEXT_ANALYTICS_REGION = TokenLibrary.getSecret(keyvault_name,\"TEXTANALYTICSREGION\",\"KeyVaultLinkedService\")\n",
        "# Translator\n",
        "TRANSLATOR_KEY = TokenLibrary.getSecret(keyvault_name,\"TRANSLATORKEY\",\"KeyVaultLinkedService\")\n",
        "TRANSLATOR_ENDPOINT = TokenLibrary.getSecret(keyvault_name,\"TRANSLATORENDPOINT\",\"KeyVaultLinkedService\")\n",
        "TRANSLATOR_REGION = TokenLibrary.getSecret(keyvault_name,\"TRANSLATORREGION\",\"KeyVaultLinkedService\")\n",
        "# Twitter\n",
        "TWITTER_API_KEY = TokenLibrary.getSecret(keyvault_name,\"TWITTERAPIKEY\",\"KeyVaultLinkedService\")\n",
        "TWITTER_API_SECRET_KEY = TokenLibrary.getSecret(keyvault_name,\"TWITTERAPISECRETKEY\",\"KeyVaultLinkedService\")\n",
        "TWITTER_ACCESS_TOKEN = TokenLibrary.getSecret(keyvault_name,\"TWITTERACCESSTOKEN\",\"KeyVaultLinkedService\")\n",
        "TWITTER_ACCESS_TOKEN_SECRET = TokenLibrary.getSecret(keyvault_name,\"TWITTERACCESSTOKENSECRET\",\"KeyVaultLinkedService\")\n",
        "TWITTER_API_AUTH = {\n",
        "  'consumer_key': TWITTER_API_KEY,\n",
        "  'consumer_secret': TWITTER_API_SECRET_KEY,\n",
        "  'access_token': TWITTER_ACCESS_TOKEN,\n",
        "  'access_token_secret': TWITTER_ACCESS_TOKEN_SECRET,\n",
        "}\n",
        "# Azure Maps\n",
        "MAPS_KEY = TokenLibrary.getSecret(keyvault_name,\"MAPSKEY\",\"KeyVaultLinkedService\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "last_created_at = None\n",
        "\n",
        "try:\n",
        "    sql_str = 'select max(created_at) from tweets_data where query =  ' + '\\'' + str(query) + '\\''\n",
        "    df_maxdate = spark.sql(sql_str).first()\n",
        "    last_created_at = df_maxdate[0]\n",
        "except:\n",
        "    last_created_at = None\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import sys\n",
        "import copy\n",
        "import tweepy\n",
        "import urllib.parse\n",
        "import sys, time, json, requests, uuid\n",
        "\n",
        "from tweepy import API\n",
        "from tweepy import Cursor\n",
        "from tweepy import OAuthHandler\n",
        "\n",
        "from dateutil.parser import parse\n",
        "from datetime import datetime, date, timedelta # don't import time here. It messes with the default library\n",
        "\n",
        "\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "\n",
        "def authenticate_client():\n",
        "    \"\"\"\n",
        "    Returns: text analytics client\n",
        "    \"\"\"\n",
        "    ta_credential = AzureKeyCredential(TEXT_ANALYTICS_KEY)\n",
        "    text_analytics_client = TextAnalyticsClient(\n",
        "            endpoint=TEXT_ANALYTICS_ENDPOINT, \n",
        "            credential=ta_credential)\n",
        "    return text_analytics_client\n",
        "text_analytics_client = authenticate_client()\n",
        "\n",
        "def get_sentiment(inp_text):\n",
        "    documents = [inp_text]\n",
        "    response = text_analytics_client.analyze_sentiment(documents = documents)[0]  \n",
        "    try:\n",
        "        overallscore = response.confidence_scores.positive + (0.5*response.confidence_scores.neutral) # check logic of this\n",
        "        return response.sentiment, overallscore\n",
        "    except Exception as err:\n",
        "        print(\"Encountered Sentiment exception. {}\".format(err))\n",
        "        return \"Neutral\",0.5\n",
        "def get_ner(inp_text):\n",
        "    try:\n",
        "        documents = [inp_text]\n",
        "        result = text_analytics_client.recognize_entities(documents = documents)[0]  \n",
        "        return [{\"text\": x.text, \"category\": x.category, \"subcategory\": x.subcategory, \"length\": x.length, \"offset\": x.offset, \"confidence_score\": x.confidence_score} for x in result.entities]\n",
        "    except Exception as err:\n",
        "        print(\"Encountered NER exception. {}\".format(err))\n",
        "    return []\n",
        "def get_key_phrases(inp_text):\n",
        "    try:\n",
        "      documents = [inp_text]\n",
        "      response = text_analytics_client.extract_key_phrases(documents = documents)[0] \n",
        "      if not response.is_error:\n",
        "          return response.key_phrases\n",
        "      else:\n",
        "          print(response.id, response.error)\n",
        "    except Exception as err:\n",
        "      print(\"Encountered Translation exception. {}\".format(err))\n",
        "    return []\n",
        "\n",
        "# Tweet Entities processing\n",
        "import requests\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import json\n",
        "\n",
        "def get_maps_response(inp):\n",
        "    url = \"https://atlas.microsoft.com/search/fuzzy/json?&subscription-key=\"+MAPS_KEY+\"&api-version=1.0&language=en-US&query=\"+inp\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "\n",
        "def get_translation(inp_text, to_languages):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "    inp_text: text to be translated\n",
        "    to_languages: list of languages to translate to\n",
        "    Returns: {lang_code: translation}, language code of the original text\n",
        "    Call to translator cognitive service detects language and translates to the target languages. \n",
        "    Result is a dictionary of language codes to translated text, along with the language detected.\n",
        "    \"\"\"\n",
        "    # Translator setup\n",
        "    translator_path = \"/translate\"\n",
        "    translator_url = TRANSLATOR_ENDPOINT + translator_path\n",
        "    params = {\n",
        "    \"api-version\": \"3.0\",\n",
        "    \"to\": to_languages\n",
        "    }\n",
        "    headers = {\n",
        "    'Ocp-Apim-Subscription-Key': TRANSLATOR_KEY,\n",
        "    'Ocp-Apim-Subscription-Region': TRANSLATOR_REGION,\n",
        "    'Content-type': 'application/json',\n",
        "    'X-ClientTraceId': str(uuid.uuid4())\n",
        "    }\n",
        "    # create and send request\n",
        "    body = [{\n",
        "    'text': inp_text\n",
        "    }]\n",
        "    request = requests.post(translator_url, params=params, headers=headers, json=body)\n",
        "    response = request.json()\n",
        "    \n",
        "    try:\n",
        "        from_language = response[0][\"detectedLanguage\"][\"language\"]\n",
        "        translations = response[0][\"translations\"]\n",
        "        res = {} \n",
        "        for trans in translations:\n",
        "            res[trans['to']] = trans['text']\n",
        "        \n",
        "        return res, from_language \n",
        "    except Exception as err:\n",
        "        print(\"Encountered an exception. {}\".format(err))\n",
        "        return err"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import sys\n",
        "import copy\n",
        "import tweepy\n",
        "import urllib.parse\n",
        "import sys, time, json, requests, uuid\n",
        "\n",
        "from tweepy import API\n",
        "from tweepy import Cursor\n",
        "from tweepy import OAuthHandler\n",
        "\n",
        "from dateutil.parser import parse\n",
        "from datetime import datetime, date, timedelta # don't import time here. It messes with the default library\n",
        "\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "\n",
        "auth = tweepy.OAuthHandler(TWITTER_API_KEY, TWITTER_API_SECRET_KEY)\n",
        "auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)\n",
        "auth_api = API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
        "\n",
        "def remove_emojis(data):\n",
        "    emoji = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "        \"]+\", re.UNICODE)\n",
        "    return re.sub(emoji, '', data)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators - query reference\n",
        "\n",
        "LANGUAGE_CODES={\"All\":\"\",\"Afrikaans\":\"af\",\"Arabic\":\"ar\",\"Assamese\":\"as\",\"Bangla\":\"bn\",\"Bosnian(Latin)\":\"bs\",\"Bulgarian\":\"bg\",\"Cantonese(Traditional)\":\"yue\",\"Catalan\":\"ca\",\"Chinese Simplified\":\"zh-Hans\",\"Chinese Traditional\":\"zh-Hant\",\"Croatian\":\"hr\",\"Czech\":\"cs\",\"Dari\":\"prs\",\"Danish\":\"da\",\"Dutch\":\"nl\",\"English\":\"en\",\"Estonian\":\"et\",\"Fijian\":\"fj\",\"Filipino\":\"fil\",\"Finnish\":\"fi\",\"French\":\"fr\",\"German\":\"de\",\"Greek\":\"el\",\"Gujarati\":\"gu\",\"Haitian Creole\":\"ht\",\"Hebrew\":\"he\",\"Hindi\":\"hi\",\"Hmong Daw\":\"mww\",\"Hungarian\":\"hu\",\"Icelandic\":\"is\",\"Indonesian\":\"id\",\"Irish\":\"ga\",\"Italian\":\"it\",\"Japanese\":\"ja\",\"Kannada\":\"kn\",\"Kazakh\":\"kk\",\"Klingon\":\"tlh-Latn\",\"Klingon(plqaD)\":\"tlh-Piqd\",\"Korean\":\"ko\",\"Kurdish(Central)\":\"ku\",\"Kurdish(Northern)\":\"kmr\",\"Latvian\":\"lv\",\"Lithuanian\":\"lt\",\"Malagasy\":\"mg\",\"Malay\":\"ms\",\"Malayalam\":\"ml\",\"Maltese\":\"mt\",\"Maori\":\"mi\",\"Marathi\":\"mr\",\"Norwegian\":\"nb\",\"Odia\":\"or\",\"Pashto\":\"ps\",\"Persian\":\"fa\",\"Polish\":\"pl\",\"Portuguese(Brazil)\":\"pt-br\",\"Portuguese(Portugal)\":\"pt-pt\",\"Punjabi\":\"pa\",\"Queretaro Otomi\":\"otq\",\"Romanian\":\"ro\",\"Russian\":\"ru\",\"Samoan\":\"sm\",\"Serbian(Cyrillic)\":\"sr-Cyrl\",\"Serbian(Latin)\":\"sr-Latn\",\"Slovak\":\"sk\",\"Slovenian\":\"sl\",\"Spanish\":\"es\",\"Swahili\":\"sw\",\"Swedish\":\"sv\",\"Tahitian\":\"ty\",\"Tamil\":\"ta\",\"Telugu\":\"te\",\"Thai\":\"th\",\"Tongan\":\"to\",\"Turkish\":\"tr\",\"Ukrainian\":\"uk\",\"Urdu\":\"ur\",\"Vietnamese\":\"vi\",\"Welsh\":\"cy\",\"Yucatec Maya\":\"yua\"}\n",
        "topic = topic.lower()\n",
        "query_language = LANGUAGE_CODES.get(query_language, \"\")\n",
        "target_languages = [LANGUAGE_CODES.get(lang, \"\") for lang in target_languages.split(\",\")]\n",
        "if \"en\" not in target_languages:\n",
        "  target_languages.append(\"en\") # we always include english in target languages\n",
        "num_tweets = int(num_tweets)\n",
        "max_days = int(days)\n",
        "\n",
        "regions = [\"Africa\",\"Arabian Gulf\",\"Asia\",\"Central America\",\"Europe\",\"Middle East\",\"North America\",\"Oceania\",\"South America\"]\n",
        "\n",
        "\n",
        "end_date = datetime.utcnow() - timedelta(days=maxdays)\n",
        "all_tweets = []\n",
        "all_users = []\n",
        "all_hashtags = []\n",
        "all_urls = []\n",
        "all_media = []\n",
        "all_handles = []\n",
        "count = 0\n",
        "query_search = True\n",
        "\n",
        "dt2 = datetime.now()\n",
        "ts= int(time.mktime(dt2.timetuple()))\n",
        "at=dt2.strftime(\"%m/%d/%Y, %H:%M:%S %Z\")\n",
        "\n",
        "for status in Cursor(auth_api.search, q=query, lang=language, result='recent', tweet_mode = \"extended\", include_rts='False').items(maxtweets_persearch):\n",
        "\n",
        "    tweet = {}\n",
        "    user = {}\n",
        "    tweet_obj = status._json\n",
        "    \n",
        "    if (last_created_at != None and last_created_at != '' and parse(tweet_obj[\"created_at\"]).timestamp() <= last_created_at.timestamp()):\n",
        "        #if (parse(tweet_obj[\"created_at\"]).timestamp() <= last_created_at.timestamp()): #ignore already loaded records\n",
        "        continue\n",
        "\n",
        "    user_obj = tweet_obj[\"user\"]\n",
        "    user[\"topickey\"] = topic\n",
        "    user[\"id\"] = user_obj[\"id_str\"]\n",
        "    user[\"document_type\"] = \"user\"\n",
        "    user['inserted_at'] = at\n",
        "    user['inserted_ts'] = ts\n",
        "    user['name'] = user_obj[\"name\"]\n",
        "    user['screen_name'] = user_obj[\"screen_name\"]\n",
        "    user['location'] = user_obj[\"location\"]\n",
        "    user['description'] = user_obj[\"description\"]\n",
        "    user['url'] = user_obj[\"url\"]\n",
        "    user['protected'] = user_obj[\"protected\"]\n",
        "    user['followers_count'] = user_obj[\"followers_count\"]\n",
        "    user['friends_count'] = user_obj[\"friends_count\"]\n",
        "    user['listed_count'] = user_obj[\"listed_count\"]\n",
        "    user['favourites_count'] = user_obj[\"favourites_count\"]\n",
        "    user['profile_image_url'] = user_obj[\"profile_image_url\"]\n",
        "    user['profile_image_url_https'] = user_obj[\"profile_image_url_https\"]\n",
        "    user['geo_enabled'] = user_obj[\"geo_enabled\"]\n",
        "    user['verified'] = user_obj[\"verified\"]\n",
        "    user['following'] = user_obj[\"following\"]\n",
        "    user['created_at'] = user_obj[\"created_at\"]\n",
        "\n",
        "    user[\"month_year\"] = str(str(parse(user_obj[\"created_at\"]).month) + \"_\"+str(parse(user_obj[\"created_at\"]).year))\n",
        "    user[\"country_azuremaps\"] = ''\n",
        "    user[\"country_code_azuremaps\"] = ''\n",
        "    user_location = tweet_obj[\"user\"][\"location\"]\n",
        "    if user_location != \"\" and user_location not in regions:\n",
        "        r_json = get_maps_response(user_location)\n",
        "        if r_json: # i.e. got a response\n",
        "            if r_json[\"summary\"][\"numResults\"] > 0:\n",
        "                # there is a location detected, so get the country\n",
        "                if \"address\" in r_json['results'][0].keys():\n",
        "                    top_match = r_json['results'][0][\"address\"]\n",
        "                    if \"country\" in top_match.keys() and \"countryCode\" in top_match.keys() :\n",
        "                        country = top_match[\"country\"]\n",
        "                        country_code = top_match[\"countryCode\"]\n",
        "                        user[\"country_azuremaps\"] = country\n",
        "                        user[\"country_code_azuremaps\"] = country_code\n",
        "\n",
        "    id_str = tweet_obj[\"id_str\"]\n",
        "\n",
        "    if tweet_obj['entities'] is not None:\n",
        "        for key, value in tweet_obj['entities'].items():\n",
        "            if key == 'hashtags':\n",
        "                for h in value:\n",
        "                    hashtag = {}\n",
        "                    hashtag['id'] = id_str\n",
        "                    hashtag['text'] = h['text']\n",
        "                    hashtag['created_datetime'] = datetime.now()\n",
        "                    all_hashtags.append(hashtag)\n",
        "\n",
        "    for um in tweet_obj['entities']['user_mentions']:\n",
        "        user_mention = {}\n",
        "        user_mention['id'] = id_str\n",
        "        user_mention['screen_name'] = um['screen_name']\n",
        "        user_mention['created_datetime'] = datetime.now()\n",
        "        all_handles.append(um)\n",
        "    for u in tweet_obj['entities']['urls']:\n",
        "        urls = {}\n",
        "        urls['id'] = id_str\n",
        "        urls['url'] = u['url']\n",
        "        urls['expanded_url'] = u['expanded_url']\n",
        "        urls['display_url'] = u['display_url']\n",
        "        urls['created_datetime'] = datetime.now()\n",
        "        all_urls.append(urls)\n",
        "    if 'media' in tweet_obj['entities']:\n",
        "        for m in tweet_obj['entities']['media']:\n",
        "            media = {}\n",
        "            media['id'] = id_str\n",
        "            media['media_url'] = m['media_url']\n",
        "            media['created_datetime'] = datetime.now()\n",
        "            all_media.append(media)\n",
        "\n",
        "    tweet[\"userid\"]=user[\"id\"]\n",
        "\n",
        "    dt2 = datetime.now()\n",
        "    ts = int(time.mktime(dt2.timetuple()))\n",
        "    at = dt2.strftime(\"%m/%d/%Y, %H:%M:%S %Z\")\n",
        "    tweet['inserted_at'] = at\n",
        "    tweet['inserted_ts'] = ts\n",
        "    tweet[\"originalid\"] = tweet_obj[\"id\"]\n",
        "    tweet[\"id\"] = str(int(tweet_obj[\"id_str\"])+abs(hash(topic))) # artifically creating our own ID\n",
        "    tweet[\"topickey\"] = topic\n",
        "    tweet[\"subtopic\"] = subtopic\n",
        "    tweet[\"created_at\"] = parse(tweet_obj[\"created_at\"])\n",
        "    tweet[\"created_date\"] = parse(tweet_obj[\"created_at\"]).date()\n",
        "    tweet[\"month_year\"] = str(str(parse(tweet_obj[\"created_at\"]).month) + \"_\"+str(parse(tweet_obj[\"created_at\"]).year))\n",
        "\n",
        "    tmp_text = tweet_obj[\"full_text\"].replace('\\n','. ').replace('\\r','.').replace('..','. ').replace(',.','. ').replace(';.','. ').replace('?.','. ').replace('!.','. ').replace(':.','. ').lstrip('.').lstrip(' ')\n",
        "    tmp_text = remove_emojis(tmp_text)\n",
        "    tweet[\"text\"]= tmp_text\n",
        "    tweet[\"document_type\"] = \"tweet\"\n",
        "\n",
        "    tweet[\"search_type\"]='Topic Search'  \n",
        "    tweet[\"query\"] = str(query)\n",
        "\n",
        "    tweet[\"is_quote_status\"] = tweet_obj[\"is_quote_status\"]\n",
        "    tweet[\"retweet_count\"] = tweet_obj[\"retweet_count\"]\n",
        "    tweet[\"favorite_count\"] = tweet_obj[\"favorite_count\"]\n",
        "    tweet[\"favorited\"] = tweet_obj[\"favorited\"]\n",
        "    tweet[\"retweeted\"] = tweet_obj[\"retweeted\"]\n",
        "    tweet[\"lang\"] = tweet_obj[\"lang\"]\n",
        "    tweet[\"source\"] = tweet_obj[\"source\"]\n",
        "    \n",
        "    city = 'NA'\n",
        "    country = 'NA'  \n",
        "    if tweet_obj['place'] is None:\n",
        "        city = 'NA'\n",
        "        country = 'NA'  \n",
        "    else:\n",
        "      city = tweet_obj[\"place\"]['name']\n",
        "      country = tweet_obj[\"place\"]['country']\n",
        "    \n",
        "    tweet['city'] = city\n",
        "    tweet['country'] = country\n",
        "\n",
        "    all_tweets.append(tweet)\n",
        "    all_users.append(user)\n",
        "    count +=1\n",
        "    if count > num_tweets:\n",
        "        break"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "df_tweets = spark.createDataFrame(Row(**x) for x in all_tweets)\n",
        "file_path = base_path + 'TweetsData/' + str(query) + '_' + str(topic) + foldername_suffix\n",
        "df_tweets.write.format('json').save(file_path)\n",
        "df_tweets.write.mode(\"append\").saveAsTable(\"tweets_data\")\n",
        "\n",
        "df_users = spark.createDataFrame(Row(**x) for x in all_users)\n",
        "file_path = base_path + 'TweetsUsersData/' +str(query) + '_' + str(topic) + foldername_suffix\n",
        "df_users.write.format('json').save(file_path)\n",
        "df_users = df_users.dropDuplicates(['id'])\n",
        "df_users.write.mode(\"append\").saveAsTable(\"tweets_users_data\")\n",
        "\n",
        "df_hashtags = spark.createDataFrame(Row(**x) for x in all_hashtags)\n",
        "file_path = base_path + 'TweetsHashtagData/' + str(query) + '_' + str(topic) + foldername_suffix\n",
        "df_hashtags.write.format('json').save(file_path)\n",
        "df_hashtags.write.mode(\"append\").saveAsTable(\"tweets_hashtag_data\")\n",
        "\n",
        "df_urls = spark.createDataFrame(Row(**x) for x in all_urls)\n",
        "file_path = base_path + 'TweetsURLData/' + str(query) + '_' + str(topic) + foldername_suffix\n",
        "df_urls.write.format('json').save(file_path)\n",
        "df_urls.write.mode(\"append\").saveAsTable(\"tweets_url_data\")\n",
        "\n",
        "df_media = spark.createDataFrame(Row(**x) for x in all_media)\n",
        "file_path = base_path + 'TweetsMediaData/' + str(query) + '_' + str(topic) + foldername_suffix\n",
        "df_media.write.format('json').save(file_path)\n",
        "df_media.write.mode(\"append\").saveAsTable(\"tweets_media_data\")\n",
        "\n",
        "df_handles = spark.createDataFrame(Row(**x) for x in all_handles)\n",
        "file_path = base_path + 'TweetsHandlesData/' + str(query) + '_' + str(topic) + foldername_suffix\n",
        "df_handles.write.format('json').save(file_path)\n",
        "df_handles.write.mode(\"append\").saveAsTable(\"tweets_handles_data\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "def get_inserted_date(publishedAt):\n",
        "    dt2 = datetime.now()\n",
        "    ts= int(time.mktime(dt2.timetuple()))\n",
        "    at=dt2.strftime(\"%m/%d/%Y, %H:%M:%S %Z\")\n",
        "    return(at)\n",
        "inserted_date_udf = udf(lambda publishedAt: get_inserted_date(publishedAt), returnType=StringType())\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Translations\", MapType(StringType(),StringType()), False),\n",
        "    StructField(\"QueryLanguage\", StringType(), False)])\n",
        "\n",
        "def get_translations(originaltext,target_languages): \n",
        "    translations_text, query_language = get_translation(originaltext, ast.literal_eval(target_languages))\n",
        "    return Row('Translations', 'QueryLanguage')(translations_text, query_language)\n",
        "\n",
        "get_translations_udf = udf(lambda originaltext,target_languages: get_translations(originaltext,target_languages),returnType=schema)\n",
        "\n",
        "\n",
        "df_translations = df_tweets.select(['id','text']) \\\n",
        "                .withColumn('Ttext1', get_translations_udf(col(\"text\"),lit(str(target_languages)))) \\\n",
        "                .withColumn('original_language', col(\"Ttext1.QueryLanguage\")) \\\n",
        "                .select(explode(col(\"Ttext1.Translations\")).alias(\"language\",\"TText\"),'id','text','original_language') \\\n",
        "                .withColumn('created_datetime', inserted_date_udf(col(\"id\")))\n",
        "\n",
        "df_translations = df_translations.select('id','text','TText','language','original_language','created_datetime')\n",
        "df_translations.write.mode(\"append\").saveAsTable(\"tweets_translations\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "schema = MapType(StringType(),ArrayType(MapType(StringType(),StringType())))\n",
        "        \n",
        "def get_ners(original_language,language,content,TContent):\n",
        "    named_entities = []\n",
        "    if language != \"en\":\n",
        "        if TContent !='':\n",
        "            named_entities = get_ner(content)\n",
        "            named_entity_obj = {language: named_entities}\n",
        "        else:\n",
        "            named_entity_obj = {language: named_entities}\n",
        "    else:\n",
        "        named_entities = get_ner(content) \n",
        "        named_entity_obj = {\"en\": named_entities}\n",
        "    return(named_entity_obj)\n",
        "\n",
        "get_ner_udf = udf(lambda original_language,language,content,TContent: get_ners(original_language,language,content,TContent),returnType=schema)\n",
        "\n",
        "df_ner = df_translations \\\n",
        "                .select(['id','original_language','language','text','TText']) \\\n",
        "                .withColumn('NER', get_ner_udf(col(\"original_language\"),col(\"language\"),col(\"text\"),col(\"TText\"))) \\\n",
        "                .withColumn('created_datetime', inserted_date_udf(col(\"id\"))) \\\n",
        "                .select(explode(col(\"NER\")).alias(\"language\",\"ner1\"),'id','original_language','created_datetime') \\\n",
        "                .select(explode(col(\"ner1\")).alias(\"ner2\"),'id','created_datetime','language','original_language') \\\n",
        "                .select(['id','created_datetime','language','original_language',col(\"ner2.text\").alias(\"value\"),col(\"ner2.subcategory\").alias(\"subcategory\"),col(\"ner2.offset\").alias(\"offset\"),\n",
        "                col(\"ner2.confidence_score\").alias(\"confidence_score\"), col(\"ner2.category\").alias(\"category\"),col(\"ner2.length\").alias(\"length\")])\n",
        "df_ner = df_ner.select('id','language','original_language','created_datetime','value','category','subcategory','offset','confidence_score','length')\n",
        "df_ner.write.mode(\"append\").saveAsTable(\"tweets_entities\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from pyspark.sql import Row\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"sentiment\", StringType(), False),\n",
        "    StructField(\"sentiment_score\", DecimalType(), False)])\n",
        "\n",
        "def get_sentiments(original_language,title,TText): \n",
        "    if original_language != \"en\":\n",
        "        sentiment, sentiment_score = get_sentiment(TText)\n",
        "    else:\n",
        "        sentiment, sentiment_score = get_sentiment(title)\n",
        "    return Row('sentiment', 'sentiment_score')(sentiment, sentiment_score)\n",
        "\n",
        "get_sentiments_udf = udf(lambda originaltext,title,TText: get_sentiments(originaltext,title,TText),returnType=schema)\n",
        "\n",
        "df_sentiments = df_translations \\\n",
        "                .select(['id','original_language','text','TText']) \\\n",
        "                .withColumn('TS1', get_sentiments_udf(col(\"original_language\"),col(\"text\"),col(\"TText\"))) \\\n",
        "                .withColumn('sentiment', col(\"TS1.sentiment\")) \\\n",
        "                .withColumn('sentiment_score', col(\"TS1.sentiment_score\")) \\\n",
        "                .withColumn('created_datetime', inserted_date_udf(col(\"id\"))) \\\n",
        "                .select(['id','sentiment','sentiment_score','created_datetime'])\n",
        "df_sentiments.write.mode(\"append\").saveAsTable(\"tweets_sentiments\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}